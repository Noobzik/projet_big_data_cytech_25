[info] welcome to sbt 1.11.7 (Eclipse Adoptium Java 11.0.29)
[info] loading project definition from /Users/zigzeug/Documents/GitHub/projet_big_data_cytech_25/ex02_data_ingestion/project
[info] loading settings for project ex02_data_ingestion from build.sbt...
[info] set current project to ex02_data_ingestion (in build file:/Users/zigzeug/Documents/GitHub/projet_big_data_cytech_25/ex02_data_ingestion/)
[info] running (fork) DataValidation 
[error] Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
[error] 25/11/30 15:42:40 WARN Utils: Your hostname, MacBook-Pro-von-Abdoulaye.local resolves to a loopback address: 127.0.0.1; using 10.188.186.236 instead (on interface en0)
[error] 25/11/30 15:42:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[error] 25/11/30 15:42:41 INFO SparkContext: Running Spark version 3.5.0
[error] 25/11/30 15:42:41 INFO SparkContext: OS info Mac OS X, 26.2, x86_64
[error] 25/11/30 15:42:41 INFO SparkContext: Java version 11.0.29
[error] 25/11/30 15:42:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[error] 25/11/30 15:42:41 INFO ResourceUtils: ==============================================================
[error] 25/11/30 15:42:41 INFO ResourceUtils: No custom resources configured for spark.driver.
[error] 25/11/30 15:42:41 INFO ResourceUtils: ==============================================================
[error] 25/11/30 15:42:41 INFO SparkContext: Submitted application: DataIngestion
[error] 25/11/30 15:42:41 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[error] 25/11/30 15:42:41 INFO ResourceProfile: Limiting resource is cpu
[error] 25/11/30 15:42:41 INFO ResourceProfileManager: Added ResourceProfile id: 0
[error] 25/11/30 15:42:41 INFO SecurityManager: Changing view acls to: zigzeug
[error] 25/11/30 15:42:41 INFO SecurityManager: Changing modify acls to: zigzeug
[error] 25/11/30 15:42:41 INFO SecurityManager: Changing view acls groups to: 
[error] 25/11/30 15:42:41 INFO SecurityManager: Changing modify acls groups to: 
[error] 25/11/30 15:42:41 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: zigzeug; groups with view permissions: EMPTY; users with modify permissions: zigzeug; groups with modify permissions: EMPTY
[error] 25/11/30 15:42:42 INFO Utils: Successfully started service 'sparkDriver' on port 51741.
[error] 25/11/30 15:42:42 INFO SparkEnv: Registering MapOutputTracker
[error] WARNING: An illegal reflective access operation has occurred
[error] WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/zigzeug/Documents/GitHub/projet_big_data_cytech_25/ex02_data_ingestion/target/bg-jobs/sbt_66fa9f52/target/75205caf/8fdd765c/spark-unsafe_2.13-3.5.0.jar) to constructor java.nio.DirectByteBuffer(long,int)
[error] WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[error] WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[error] WARNING: All illegal access operations will be denied in a future release
[error] 25/11/30 15:42:42 INFO SparkEnv: Registering BlockManagerMaster
[error] 25/11/30 15:42:42 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[error] 25/11/30 15:42:42 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[error] 25/11/30 15:42:42 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[error] 25/11/30 15:42:42 INFO DiskBlockManager: Created local directory at /private/var/folders/h1/pkmwsfd12hz6_rnssw0zx33w0000gn/T/blockmgr-23064587-fa01-4859-8e41-05cd4be4d5dd
[error] 25/11/30 15:42:42 INFO MemoryStore: MemoryStore started with capacity 2.2 GiB
[error] 25/11/30 15:42:42 INFO SparkEnv: Registering OutputCommitCoordinator
[error] 25/11/30 15:42:42 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[error] 25/11/30 15:42:42 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[error] 25/11/30 15:42:42 INFO Executor: Starting executor ID driver on host 10.188.186.236
[error] 25/11/30 15:42:42 INFO Executor: OS info Mac OS X, 26.2, x86_64
[error] 25/11/30 15:42:42 INFO Executor: Java version 11.0.29
[error] 25/11/30 15:42:42 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[error] 25/11/30 15:42:42 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@67f77f6e for default.
[error] 25/11/30 15:42:42 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51742.
[error] 25/11/30 15:42:42 INFO NettyBlockTransferService: Server created on 10.188.186.236:51742
[error] 25/11/30 15:42:42 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[error] 25/11/30 15:42:42 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.188.186.236, 51742, None)
[error] 25/11/30 15:42:42 INFO BlockManagerMasterEndpoint: Registering block manager 10.188.186.236:51742 with 2.2 GiB RAM, BlockManagerId(driver, 10.188.186.236, 51742, None)
[error] 25/11/30 15:42:42 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.188.186.236, 51742, None)
[error] 25/11/30 15:42:42 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.188.186.236, 51742, None)
[info] Reading data from ../data/raw/yellow_tripdata_2025-01.parquet
[error] 25/11/30 15:42:43 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[error] 25/11/30 15:42:43 INFO SharedState: Warehouse path is 'file:/Users/zigzeug/Documents/GitHub/projet_big_data_cytech_25/ex02_data_ingestion/spark-warehouse'.
[error] 25/11/30 15:42:45 INFO InMemoryFileIndex: It took 69 ms to list leaf files for 1 paths.
[error] 25/11/30 15:42:45 INFO SparkContext: Starting job: parquet at DataValidation.scala:29
[error] 25/11/30 15:42:45 INFO DAGScheduler: Got job 0 (parquet at DataValidation.scala:29) with 1 output partitions
[error] 25/11/30 15:42:45 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at DataValidation.scala:29)
[error] 25/11/30 15:42:45 INFO DAGScheduler: Parents of final stage: List()
[error] 25/11/30 15:42:45 INFO DAGScheduler: Missing parents: List()
[error] 25/11/30 15:42:45 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at DataValidation.scala:29), which has no missing parents
[error] 25/11/30 15:42:46 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 102.8 KiB, free 2.2 GiB)
[error] 25/11/30 15:42:46 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 36.9 KiB, free 2.2 GiB)
[error] 25/11/30 15:42:46 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.188.186.236:51742 (size: 36.9 KiB, free: 2.2 GiB)
[error] 25/11/30 15:42:46 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580
[error] 25/11/30 15:42:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at DataValidation.scala:29) (first 15 tasks are for partitions Vector(0))
[error] 25/11/30 15:42:46 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[error] 25/11/30 15:42:46 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.188.186.236, executor driver, partition 0, PROCESS_LOCAL, 7957 bytes) 
[error] 25/11/30 15:42:46 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[error] 25/11/30 15:42:47 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2724 bytes result sent to driver
[error] 25/11/30 15:42:47 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 639 ms on 10.188.186.236 (executor driver) (1/1)
[error] 25/11/30 15:42:47 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
[error] 25/11/30 15:42:47 INFO DAGScheduler: ResultStage 0 (parquet at DataValidation.scala:29) finished in 1.436 s
[error] 25/11/30 15:42:47 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[error] 25/11/30 15:42:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[error] 25/11/30 15:42:47 INFO DAGScheduler: Job 0 finished: parquet at DataValidation.scala:29, took 1.502838 s
[error] 25/11/30 15:42:47 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.188.186.236:51742 in memory (size: 36.9 KiB, free: 2.2 GiB)
[error] 25/11/30 15:42:49 INFO FileSourceStrategy: Pushed Filters: 
[error] 25/11/30 15:42:49 INFO FileSourceStrategy: Post-Scan Filters: 
[error] 25/11/30 15:42:49 INFO CodeGenerator: Code generated in 313.411924 ms
[error] 25/11/30 15:42:49 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 200.0 KiB, free 2.2 GiB)
[error] 25/11/30 15:42:49 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 2.2 GiB)
[error] 25/11/30 15:42:49 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.188.186.236:51742 (size: 34.6 KiB, free: 2.2 GiB)
[error] 25/11/30 15:42:49 INFO SparkContext: Created broadcast 1 from count at DataValidation.scala:31
[error] 25/11/30 15:42:49 INFO FileSourceScanExec: Planning scan with bin packing, max size: 7919067 bytes, open cost is considered as scanning 4194304 bytes.
[error] 25/11/30 15:42:49 INFO DAGScheduler: Registering RDD 5 (count at DataValidation.scala:31) as input to shuffle 0
[error] 25/11/30 15:42:49 INFO DAGScheduler: Got map stage job 1 (count at DataValidation.scala:31) with 8 output partitions
[error] 25/11/30 15:42:49 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (count at DataValidation.scala:31)
[error] 25/11/30 15:42:49 INFO DAGScheduler: Parents of final stage: List()
[error] 25/11/30 15:42:49 INFO DAGScheduler: Missing parents: List()
[error] 25/11/30 15:42:49 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at count at DataValidation.scala:31), which has no missing parents
[error] 25/11/30 15:42:49 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 17.5 KiB, free 2.2 GiB)
[error] 25/11/30 15:42:49 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.9 KiB, free 2.2 GiB)
[error] 25/11/30 15:42:49 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.188.186.236:51742 (size: 7.9 KiB, free: 2.2 GiB)
[error] 25/11/30 15:42:49 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1580
[error] 25/11/30 15:42:49 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at count at DataValidation.scala:31) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
[error] 25/11/30 15:42:49 INFO TaskSchedulerImpl: Adding task set 1.0 with 8 tasks resource profile 0
[error] 25/11/30 15:42:49 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.188.186.236, executor driver, partition 0, PROCESS_LOCAL, 8352 bytes) 
[error] 25/11/30 15:42:49 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (10.188.186.236, executor driver, partition 1, PROCESS_LOCAL, 8352 bytes) 
[error] 25/11/30 15:42:49 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (10.188.186.236, executor driver, partition 2, PROCESS_LOCAL, 8352 bytes) 
[error] 25/11/30 15:42:49 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (10.188.186.236, executor driver, partition 3, PROCESS_LOCAL, 8352 bytes) 
[error] 25/11/30 15:42:49 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5) (10.188.186.236, executor driver, partition 4, PROCESS_LOCAL, 8352 bytes) 
[error] 25/11/30 15:42:49 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6) (10.188.186.236, executor driver, partition 5, PROCESS_LOCAL, 8352 bytes) 
[error] 25/11/30 15:42:49 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7) (10.188.186.236, executor driver, partition 6, PROCESS_LOCAL, 8352 bytes) 
[error] 25/11/30 15:42:49 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8) (10.188.186.236, executor driver, partition 7, PROCESS_LOCAL, 8352 bytes) 
[error] 25/11/30 15:42:49 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[error] 25/11/30 15:42:49 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)
[error] 25/11/30 15:42:49 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)
[error] 25/11/30 15:42:49 INFO Executor: Running task 4.0 in stage 1.0 (TID 5)
[error] 25/11/30 15:42:49 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)
[error] 25/11/30 15:42:49 INFO Executor: Running task 6.0 in stage 1.0 (TID 7)
[error] 25/11/30 15:42:49 INFO Executor: Running task 7.0 in stage 1.0 (TID 8)
[error] 25/11/30 15:42:49 INFO Executor: Running task 5.0 in stage 1.0 (TID 6)
[error] 25/11/30 15:42:49 INFO CodeGenerator: Code generated in 23.541115 ms
[error] 25/11/30 15:42:50 INFO FileScanRDD: Reading File path: file:///Users/zigzeug/Documents/GitHub/projet_big_data_cytech_25/data/raw/yellow_tripdata_2025-01.parquet, range: 7919067-15838134, partition values: [empty row]
[error] 25/11/30 15:42:50 INFO FileScanRDD: Reading File path: file:///Users/zigzeug/Documents/GitHub/projet_big_data_cytech_25/data/raw/yellow_tripdata_2025-01.parquet, range: 55433469-59158238, partition values: [empty row]
[error] 25/11/30 15:42:50 INFO FileScanRDD: Reading File path: file:///Users/zigzeug/Documents/GitHub/projet_big_data_cytech_25/data/raw/yellow_tripdata_2025-01.parquet, range: 39595335-47514402, partition values: [empty row]
[error] 25/11/30 15:42:50 INFO FileScanRDD: Reading File path: file:///Users/zigzeug/Documents/GitHub/projet_big_data_cytech_25/data/raw/yellow_tripdata_2025-01.parquet, range: 15838134-23757201, partition values: [empty row]
[error] 25/11/30 15:42:50 INFO FileScanRDD: Reading File path: file:///Users/zigzeug/Documents/GitHub/projet_big_data_cytech_25/data/raw/yellow_tripdata_2025-01.parquet, range: 31676268-39595335, partition values: [empty row]
[error] 25/11/30 15:42:50 INFO FileScanRDD: Reading File path: file:///Users/zigzeug/Documents/GitHub/projet_big_data_cytech_25/data/raw/yellow_tripdata_2025-01.parquet, range: 23757201-31676268, partition values: [empty row]
[error] 25/11/30 15:42:50 INFO FileScanRDD: Reading File path: file:///Users/zigzeug/Documents/GitHub/projet_big_data_cytech_25/data/raw/yellow_tripdata_2025-01.parquet, range: 0-7919067, partition values: [empty row]
[error] 25/11/30 15:42:50 INFO FileScanRDD: Reading File path: file:///Users/zigzeug/Documents/GitHub/projet_big_data_cytech_25/data/raw/yellow_tripdata_2025-01.parquet, range: 47514402-55433469, partition values: [empty row]
[error] 25/11/30 15:42:50 INFO Executor: Finished task 4.0 in stage 1.0 (TID 5). 2227 bytes result sent to driver
[error] 25/11/30 15:42:50 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2227 bytes result sent to driver
[error] 25/11/30 15:42:50 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2227 bytes result sent to driver
[error] 25/11/30 15:42:50 INFO Executor: Finished task 6.0 in stage 1.0 (TID 7). 2227 bytes result sent to driver
[error] 25/11/30 15:42:50 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 344 ms on 10.188.186.236 (executor driver) (1/8)
[error] 25/11/30 15:42:50 INFO Executor: Finished task 7.0 in stage 1.0 (TID 8). 2270 bytes result sent to driver
[error] 25/11/30 15:42:50 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 340 ms on 10.188.186.236 (executor driver) (2/8)
[error] 25/11/30 15:42:50 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 339 ms on 10.188.186.236 (executor driver) (3/8)
[error] 25/11/30 15:42:50 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 338 ms on 10.188.186.236 (executor driver) (4/8)
[error] 25/11/30 15:42:50 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 340 ms on 10.188.186.236 (executor driver) (5/8)
[error] 25/11/30 15:42:50 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 2270 bytes result sent to driver
[error] 25/11/30 15:42:50 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 413 ms on 10.188.186.236 (executor driver) (6/8)
[error] 25/11/30 15:42:50 INFO Executor: Finished task 5.0 in stage 1.0 (TID 6). 2270 bytes result sent to driver
[error] 25/11/30 15:42:50 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 412 ms on 10.188.186.236 (executor driver) (7/8)
[error] 25/11/30 15:42:50 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2270 bytes result sent to driver
[error] 25/11/30 15:42:50 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 417 ms on 10.188.186.236 (executor driver) (8/8)
[error] 25/11/30 15:42:50 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
[error] 25/11/30 15:42:50 INFO DAGScheduler: ShuffleMapStage 1 (count at DataValidation.scala:31) finished in 0.474 s
[error] 25/11/30 15:42:50 INFO DAGScheduler: looking for newly runnable stages
[error] 25/11/30 15:42:50 INFO DAGScheduler: running: HashSet()
[error] 25/11/30 15:42:50 INFO DAGScheduler: waiting: HashSet()
[error] 25/11/30 15:42:50 INFO DAGScheduler: failed: HashSet()
[error] 25/11/30 15:42:50 INFO CodeGenerator: Code generated in 16.860487 ms
[error] 25/11/30 15:42:50 INFO SparkContext: Starting job: count at DataValidation.scala:31
[error] 25/11/30 15:42:50 INFO DAGScheduler: Got job 2 (count at DataValidation.scala:31) with 1 output partitions
[error] 25/11/30 15:42:50 INFO DAGScheduler: Final stage: ResultStage 3 (count at DataValidation.scala:31)
[error] 25/11/30 15:42:50 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
[error] 25/11/30 15:42:50 INFO DAGScheduler: Missing parents: List()
[error] 25/11/30 15:42:50 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[8] at count at DataValidation.scala:31), which has no missing parents
[error] 25/11/30 15:42:50 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 13.1 KiB, free 2.2 GiB)
[error] 25/11/30 15:42:50 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 2.2 GiB)
[error] 25/11/30 15:42:50 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.188.186.236:51742 (size: 6.1 KiB, free: 2.2 GiB)
[error] 25/11/30 15:42:50 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1580
[error] 25/11/30 15:42:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[8] at count at DataValidation.scala:31) (first 15 tasks are for partitions Vector(0))
[error] 25/11/30 15:42:50 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[error] 25/11/30 15:42:50 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 9) (10.188.186.236, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
[error] 25/11/30 15:42:50 INFO Executor: Running task 0.0 in stage 3.0 (TID 9)
[error] 25/11/30 15:42:50 INFO ShuffleBlockFetcherIterator: Getting 8 (480.0 B) non-empty blocks including 8 (480.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[error] 25/11/30 15:42:50 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
[error] 25/11/30 15:42:50 INFO CodeGenerator: Code generated in 12.100561 ms
[error] 25/11/30 15:42:50 INFO Executor: Finished task 0.0 in stage 3.0 (TID 9). 4127 bytes result sent to driver
[error] 25/11/30 15:42:50 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 9) in 99 ms on 10.188.186.236 (executor driver) (1/1)
[error] 25/11/30 15:42:50 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
[error] 25/11/30 15:42:50 INFO DAGScheduler: ResultStage 3 (count at DataValidation.scala:31) finished in 0.115 s
[error] 25/11/30 15:42:50 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[error] 25/11/30 15:42:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[error] 25/11/30 15:42:50 INFO DAGScheduler: Job 2 finished: count at DataValidation.scala:31, took 0.127498 s
[info] Total records: 3475226
[error] 25/11/30 15:42:50 INFO FileSourceStrategy: Pushed Filters: IsNotNull(passenger_count),IsNotNull(trip_distance),IsNotNull(total_amount),IsNotNull(tpep_pickup_datetime),IsNotNull(tpep_dropoff_datetime),GreaterThan(passenger_count,0),GreaterThanOrEqual(trip_distance,0.0),GreaterThanOrEqual(total_amount,0.0)
[error] 25/11/30 15:42:50 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(passenger_count#3L),isnotnull(trip_distance#4),isnotnull(total_amount#16),isnotnull(tpep_pickup_datetime#1),isnotnull(tpep_dropoff_datetime#2),(passenger_count#3L > 0),(trip_distance#4 >= 0.0),(total_amount#16 >= 0.0),(tpep_pickup_datetime#1 < tpep_dropoff_datetime#2)
[error] 25/11/30 15:42:50 INFO CodeGenerator: Code generated in 39.743505 ms
[error] 25/11/30 15:42:50 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 200.7 KiB, free 2.2 GiB)
[error] 25/11/30 15:42:50 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 2.2 GiB)
[error] 25/11/30 15:42:50 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.188.186.236:51742 (size: 35.0 KiB, free: 2.2 GiB)
[error] 25/11/30 15:42:50 INFO SparkContext: Created broadcast 4 from count at DataValidation.scala:42
[error] 25/11/30 15:42:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 7919067 bytes, open cost is considered as scanning 4194304 bytes.
[error] 25/11/30 15:42:50 INFO DAGScheduler: Registering RDD 12 (count at DataValidation.scala:42) as input to shuffle 1
[error] 25/11/30 15:42:50 INFO DAGScheduler: Got map stage job 3 (count at DataValidation.scala:42) with 8 output partitions
[error] 25/11/30 15:42:50 INFO DAGScheduler: Final stage: ShuffleMapStage 4 (count at DataValidation.scala:42)
[error] 25/11/30 15:42:50 INFO DAGScheduler: Parents of final stage: List()
[error] 25/11/30 15:42:50 INFO DAGScheduler: Missing parents: List()
[error] 25/11/30 15:42:50 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[12] at count at DataValidation.scala:42), which has no missing parents
[error] 25/11/30 15:42:50 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 22.0 KiB, free 2.2 GiB)
[error] 25/11/30 15:42:50 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 9.0 KiB, free 2.2 GiB)
[error] 25/11/30 15:42:50 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.188.186.236:51742 (size: 9.0 KiB, free: 2.2 GiB)
[error] 25/11/30 15:42:50 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1580
[error] 25/11/30 15:42:50 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[12] at count at DataValidation.scala:42) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
[error] 25/11/30 15:42:50 INFO TaskSchedulerImpl: Adding task set 4.0 with 8 tasks resource profile 0
[error] 25/11/30 15:42:50 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 10) (10.188.186.236, executor driver, partition 0, PROCESS_LOCAL, 8352 bytes) 
[error] 25/11/30 15:42:50 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 11) (10.188.186.236, executor driver, partition 1, PROCESS_LOCAL, 8352 bytes) 
[error] 25/11/30 15:42:50 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 12) (10.188.186.236, executor driver, partition 2, PROCESS_LOCAL, 8352 bytes) 
[error] 25/11/30 15:42:50 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 13) (10.188.186.236, executor driver, partition 3, PROCESS_LOCAL, 8352 bytes) 
[error] 25/11/30 15:42:50 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 14) (10.188.186.236, executor driver, partition 4, PROCESS_LOCAL, 8352 bytes) 
[error] 25/11/30 15:42:50 INFO TaskSetManager: Starting task 5.0 in stage 4.0 (TID 15) (10.188.186.236, executor driver, partition 5, PROCESS_LOCAL, 8352 bytes) 
[error] 25/11/30 15:42:50 INFO TaskSetManager: Starting task 6.0 in stage 4.0 (TID 16) (10.188.186.236, executor driver, partition 6, PROCESS_LOCAL, 8352 bytes) 
[error] 25/11/30 15:42:50 INFO TaskSetManager: Starting task 7.0 in stage 4.0 (TID 17) (10.188.186.236, executor driver, partition 7, PROCESS_LOCAL, 8352 bytes) 
[error] 25/11/30 15:42:50 INFO Executor: Running task 1.0 in stage 4.0 (TID 11)
[error] 25/11/30 15:42:50 INFO Executor: Running task 2.0 in stage 4.0 (TID 12)
[error] 25/11/30 15:42:50 INFO Executor: Running task 4.0 in stage 4.0 (TID 14)
[error] 25/11/30 15:42:50 INFO Executor: Running task 5.0 in stage 4.0 (TID 15)
[error] 25/11/30 15:42:50 INFO Executor: Running task 3.0 in stage 4.0 (TID 13)
[error] 25/11/30 15:42:50 INFO Executor: Running task 7.0 in stage 4.0 (TID 17)
[error] 25/11/30 15:42:50 INFO Executor: Running task 6.0 in stage 4.0 (TID 16)
[error] 25/11/30 15:42:50 INFO Executor: Running task 0.0 in stage 4.0 (TID 10)
[error] 25/11/30 15:42:50 INFO CodeGenerator: Code generated in 27.464476 ms
[error] 25/11/30 15:42:50 INFO FileScanRDD: Reading File path: file:///Users/zigzeug/Documents/GitHub/projet_big_data_cytech_25/data/raw/yellow_tripdata_2025-01.parquet, range: 39595335-47514402, partition values: [empty row]
[error] 25/11/30 15:42:50 INFO FileScanRDD: Reading File path: file:///Users/zigzeug/Documents/GitHub/projet_big_data_cytech_25/data/raw/yellow_tripdata_2025-01.parquet, range: 15838134-23757201, partition values: [empty row]
[error] 25/11/30 15:42:50 INFO FileScanRDD: Reading File path: file:///Users/zigzeug/Documents/GitHub/projet_big_data_cytech_25/data/raw/yellow_tripdata_2025-01.parquet, range: 7919067-15838134, partition values: [empty row]
[error] 25/11/30 15:42:50 INFO FileScanRDD: Reading File path: file:///Users/zigzeug/Documents/GitHub/projet_big_data_cytech_25/data/raw/yellow_tripdata_2025-01.parquet, range: 47514402-55433469, partition values: [empty row]
[error] 25/11/30 15:42:50 INFO FileScanRDD: Reading File path: file:///Users/zigzeug/Documents/GitHub/projet_big_data_cytech_25/data/raw/yellow_tripdata_2025-01.parquet, range: 31676268-39595335, partition values: [empty row]
[error] 25/11/30 15:42:50 INFO FileScanRDD: Reading File path: file:///Users/zigzeug/Documents/GitHub/projet_big_data_cytech_25/data/raw/yellow_tripdata_2025-01.parquet, range: 55433469-59158238, partition values: [empty row]
[error] 25/11/30 15:42:50 INFO FileScanRDD: Reading File path: file:///Users/zigzeug/Documents/GitHub/projet_big_data_cytech_25/data/raw/yellow_tripdata_2025-01.parquet, range: 0-7919067, partition values: [empty row]
[error] 25/11/30 15:42:50 INFO FileScanRDD: Reading File path: file:///Users/zigzeug/Documents/GitHub/projet_big_data_cytech_25/data/raw/yellow_tripdata_2025-01.parquet, range: 23757201-31676268, partition values: [empty row]
[error] 25/11/30 15:42:50 INFO FilterCompat: Filtering using predicate: and(and(and(and(and(noteq(passenger_count, null), noteq(trip_distance, null)), noteq(total_amount, null)), gt(passenger_count, 0)), gteq(trip_distance, 0.0)), gteq(total_amount, 0.0))
[error] 25/11/30 15:42:50 INFO FilterCompat: Filtering using predicate: and(and(and(and(and(noteq(passenger_count, null), noteq(trip_distance, null)), noteq(total_amount, null)), gt(passenger_count, 0)), gteq(trip_distance, 0.0)), gteq(total_amount, 0.0))
[error] 25/11/30 15:42:50 INFO FilterCompat: Filtering using predicate: and(and(and(and(and(noteq(passenger_count, null), noteq(trip_distance, null)), noteq(total_amount, null)), gt(passenger_count, 0)), gteq(trip_distance, 0.0)), gteq(total_amount, 0.0))
[error] 25/11/30 15:42:50 INFO FilterCompat: Filtering using predicate: and(and(and(and(and(noteq(passenger_count, null), noteq(trip_distance, null)), noteq(total_amount, null)), gt(passenger_count, 0)), gteq(trip_distance, 0.0)), gteq(total_amount, 0.0))
[error] 25/11/30 15:42:50 INFO FilterCompat: Filtering using predicate: and(and(and(and(and(noteq(passenger_count, null), noteq(trip_distance, null)), noteq(total_amount, null)), gt(passenger_count, 0)), gteq(trip_distance, 0.0)), gteq(total_amount, 0.0))
[error] 25/11/30 15:42:50 INFO FilterCompat: Filtering using predicate: and(and(and(and(and(noteq(passenger_count, null), noteq(trip_distance, null)), noteq(total_amount, null)), gt(passenger_count, 0)), gteq(trip_distance, 0.0)), gteq(total_amount, 0.0))
[error] 25/11/30 15:42:50 INFO FilterCompat: Filtering using predicate: and(and(and(and(and(noteq(passenger_count, null), noteq(trip_distance, null)), noteq(total_amount, null)), gt(passenger_count, 0)), gteq(trip_distance, 0.0)), gteq(total_amount, 0.0))
[error] 25/11/30 15:42:50 INFO FilterCompat: Filtering using predicate: and(and(and(and(and(noteq(passenger_count, null), noteq(trip_distance, null)), noteq(total_amount, null)), gt(passenger_count, 0)), gteq(trip_distance, 0.0)), gteq(total_amount, 0.0))
[error] 25/11/30 15:42:50 INFO Executor: Finished task 7.0 in stage 4.0 (TID 17). 2283 bytes result sent to driver
[error] 25/11/30 15:42:50 INFO TaskSetManager: Finished task 7.0 in stage 4.0 (TID 17) in 123 ms on 10.188.186.236 (executor driver) (1/8)
[error] 25/11/30 15:42:50 INFO Executor: Finished task 4.0 in stage 4.0 (TID 14). 2283 bytes result sent to driver
[error] 25/11/30 15:42:50 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 14) in 127 ms on 10.188.186.236 (executor driver) (2/8)
[error] 25/11/30 15:42:50 INFO Executor: Finished task 0.0 in stage 4.0 (TID 10). 2283 bytes result sent to driver
[error] 25/11/30 15:42:50 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 10) in 131 ms on 10.188.186.236 (executor driver) (3/8)
[error] 25/11/30 15:42:50 INFO Executor: Finished task 6.0 in stage 4.0 (TID 16). 2283 bytes result sent to driver
[error] 25/11/30 15:42:50 INFO TaskSetManager: Finished task 6.0 in stage 4.0 (TID 16) in 130 ms on 10.188.186.236 (executor driver) (4/8)
[error] 25/11/30 15:42:50 INFO CodecPool: Got brand-new decompressor [.zstd]
[error] 25/11/30 15:42:50 INFO CodecPool: Got brand-new decompressor [.zstd]
[error] 25/11/30 15:42:50 INFO CodecPool: Got brand-new decompressor [.zstd]
[error] 25/11/30 15:42:50 INFO Executor: Finished task 2.0 in stage 4.0 (TID 12). 2283 bytes result sent to driver
[error] 25/11/30 15:42:50 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 12) in 135 ms on 10.188.186.236 (executor driver) (5/8)
[error] 25/11/30 15:42:51 INFO ColumnIndexFilter: No offset index for column passenger_count is available; Unable to do filtering
[error] 25/11/30 15:42:51 INFO ColumnIndexFilter: No offset index for column passenger_count is available; Unable to do filtering
[error] 25/11/30 15:42:51 INFO ColumnIndexFilter: No offset index for column passenger_count is available; Unable to do filtering
[error] 25/11/30 15:42:51 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 10.188.186.236:51742 in memory (size: 6.1 KiB, free: 2.2 GiB)
[error] 25/11/30 15:42:51 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.188.186.236:51742 in memory (size: 34.6 KiB, free: 2.2 GiB)
[error] 25/11/30 15:42:51 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.188.186.236:51742 in memory (size: 7.9 KiB, free: 2.2 GiB)
[error] 25/11/30 15:42:52 INFO Executor: Finished task 5.0 in stage 4.0 (TID 15). 2326 bytes result sent to driver
[error] 25/11/30 15:42:52 INFO TaskSetManager: Finished task 5.0 in stage 4.0 (TID 15) in 2055 ms on 10.188.186.236 (executor driver) (6/8)
[error] 25/11/30 15:42:52 INFO Executor: Finished task 3.0 in stage 4.0 (TID 13). 2326 bytes result sent to driver
[error] 25/11/30 15:42:52 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 13) in 2091 ms on 10.188.186.236 (executor driver) (7/8)
[error] 25/11/30 15:42:52 INFO Executor: Finished task 1.0 in stage 4.0 (TID 11). 2326 bytes result sent to driver
[error] 25/11/30 15:42:52 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 11) in 2101 ms on 10.188.186.236 (executor driver) (8/8)
[error] 25/11/30 15:42:52 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
[error] 25/11/30 15:42:52 INFO DAGScheduler: ShuffleMapStage 4 (count at DataValidation.scala:42) finished in 2.118 s
[error] 25/11/30 15:42:52 INFO DAGScheduler: looking for newly runnable stages
[error] 25/11/30 15:42:52 INFO DAGScheduler: running: HashSet()
[error] 25/11/30 15:42:52 INFO DAGScheduler: waiting: HashSet()
[error] 25/11/30 15:42:52 INFO DAGScheduler: failed: HashSet()
[error] 25/11/30 15:42:52 INFO SparkContext: Starting job: count at DataValidation.scala:42
[error] 25/11/30 15:42:52 INFO DAGScheduler: Got job 4 (count at DataValidation.scala:42) with 1 output partitions
[error] 25/11/30 15:42:52 INFO DAGScheduler: Final stage: ResultStage 6 (count at DataValidation.scala:42)
[error] 25/11/30 15:42:52 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
[error] 25/11/30 15:42:52 INFO DAGScheduler: Missing parents: List()
[error] 25/11/30 15:42:52 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[15] at count at DataValidation.scala:42), which has no missing parents
[error] 25/11/30 15:42:52 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 13.1 KiB, free 2.2 GiB)
[error] 25/11/30 15:42:52 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 2.2 GiB)
[error] 25/11/30 15:42:52 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.188.186.236:51742 (size: 6.1 KiB, free: 2.2 GiB)
[error] 25/11/30 15:42:52 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1580
[error] 25/11/30 15:42:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[15] at count at DataValidation.scala:42) (first 15 tasks are for partitions Vector(0))
[error] 25/11/30 15:42:52 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[error] 25/11/30 15:42:52 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 18) (10.188.186.236, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
[error] 25/11/30 15:42:52 INFO Executor: Running task 0.0 in stage 6.0 (TID 18)
[error] 25/11/30 15:42:52 INFO ShuffleBlockFetcherIterator: Getting 8 (480.0 B) non-empty blocks including 8 (480.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[error] 25/11/30 15:42:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[error] 25/11/30 15:42:52 INFO Executor: Finished task 0.0 in stage 6.0 (TID 18). 4084 bytes result sent to driver
[error] 25/11/30 15:42:52 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 18) in 17 ms on 10.188.186.236 (executor driver) (1/1)
[error] 25/11/30 15:42:52 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
[error] 25/11/30 15:42:52 INFO DAGScheduler: ResultStage 6 (count at DataValidation.scala:42) finished in 0.025 s
[error] 25/11/30 15:42:52 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[error] 25/11/30 15:42:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[error] 25/11/30 15:42:52 INFO DAGScheduler: Job 4 finished: count at DataValidation.scala:42, took 0.030437 s
[info] Valid records: 2849171
[error] 25/11/30 15:42:52 INFO FileSourceStrategy: Pushed Filters: 
[error] 25/11/30 15:42:52 INFO FileSourceStrategy: Post-Scan Filters: 
[error] 25/11/30 15:42:53 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 200.0 KiB, free 2.2 GiB)
[error] 25/11/30 15:42:53 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 10.188.186.236:51742 in memory (size: 6.1 KiB, free: 2.2 GiB)
[error] 25/11/30 15:42:53 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 10.188.186.236:51742 in memory (size: 35.0 KiB, free: 2.2 GiB)
[error] 25/11/30 15:42:53 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 2.2 GiB)
[error] 25/11/30 15:42:53 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 10.188.186.236:51742 in memory (size: 9.0 KiB, free: 2.2 GiB)
[error] 25/11/30 15:42:53 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.188.186.236:51742 (size: 34.6 KiB, free: 2.2 GiB)
[error] 25/11/30 15:42:53 INFO SparkContext: Created broadcast 7 from count at DataValidation.scala:44
[error] 25/11/30 15:42:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 7919067 bytes, open cost is considered as scanning 4194304 bytes.
[error] 25/11/30 15:42:53 INFO DAGScheduler: Registering RDD 19 (count at DataValidation.scala:44) as input to shuffle 2
[error] 25/11/30 15:42:53 INFO DAGScheduler: Got map stage job 5 (count at DataValidation.scala:44) with 8 output partitions
[error] 25/11/30 15:42:53 INFO DAGScheduler: Final stage: ShuffleMapStage 7 (count at DataValidation.scala:44)
[error] 25/11/30 15:42:53 INFO DAGScheduler: Parents of final stage: List()
[error] 25/11/30 15:42:53 INFO DAGScheduler: Missing parents: List()
[error] 25/11/30 15:42:53 INFO DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[19] at count at DataValidation.scala:44), which has no missing parents
[error] 25/11/30 15:42:53 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 17.5 KiB, free 2.2 GiB)
[error] 25/11/30 15:42:53 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 7.9 KiB, free 2.2 GiB)
[error] 25/11/30 15:42:53 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.188.186.236:51742 (size: 7.9 KiB, free: 2.2 GiB)
[error] 25/11/30 15:42:53 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1580
[error] 25/11/30 15:42:53 INFO DAGScheduler: Submitting 8 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[19] at count at DataValidation.scala:44) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
[error] 25/11/30 15:42:53 INFO TaskSchedulerImpl: Adding task set 7.0 with 8 tasks resource profile 0
[error] 25/11/30 15:42:53 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 19) (10.188.186.236, executor driver, partition 0, PROCESS_LOCAL, 8352 bytes) 
[error] 25/11/30 15:42:53 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 20) (10.188.186.236, executor driver, partition 1, PROCESS_LOCAL, 8352 bytes) 
[error] 25/11/30 15:42:53 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 21) (10.188.186.236, executor driver, partition 2, PROCESS_LOCAL, 8352 bytes) 
[error] 25/11/30 15:42:53 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 22) (10.188.186.236, executor driver, partition 3, PROCESS_LOCAL, 8352 bytes) 
[error] 25/11/30 15:42:53 INFO TaskSetManager: Starting task 4.0 in stage 7.0 (TID 23) (10.188.186.236, executor driver, partition 4, PROCESS_LOCAL, 8352 bytes) 
[error] 25/11/30 15:42:53 INFO TaskSetManager: Starting task 5.0 in stage 7.0 (TID 24) (10.188.186.236, executor driver, partition 5, PROCESS_LOCAL, 8352 bytes) 
[error] 25/11/30 15:42:53 INFO TaskSetManager: Starting task 6.0 in stage 7.0 (TID 25) (10.188.186.236, executor driver, partition 6, PROCESS_LOCAL, 8352 bytes) 
[error] 25/11/30 15:42:53 INFO TaskSetManager: Starting task 7.0 in stage 7.0 (TID 26) (10.188.186.236, executor driver, partition 7, PROCESS_LOCAL, 8352 bytes) 
[error] 25/11/30 15:42:53 INFO Executor: Running task 0.0 in stage 7.0 (TID 19)
[error] 25/11/30 15:42:53 INFO Executor: Running task 1.0 in stage 7.0 (TID 20)
[error] 25/11/30 15:42:53 INFO Executor: Running task 2.0 in stage 7.0 (TID 21)
[error] 25/11/30 15:42:53 INFO Executor: Running task 3.0 in stage 7.0 (TID 22)
[error] 25/11/30 15:42:53 INFO Executor: Running task 4.0 in stage 7.0 (TID 23)
[error] 25/11/30 15:42:53 INFO Executor: Running task 5.0 in stage 7.0 (TID 24)
[error] 25/11/30 15:42:53 INFO Executor: Running task 6.0 in stage 7.0 (TID 25)
[error] 25/11/30 15:42:53 INFO Executor: Running task 7.0 in stage 7.0 (TID 26)
[error] 25/11/30 15:42:53 INFO FileScanRDD: Reading File path: file:///Users/zigzeug/Documents/GitHub/projet_big_data_cytech_25/data/raw/yellow_tripdata_2025-01.parquet, range: 15838134-23757201, partition values: [empty row]
[error] 25/11/30 15:42:53 INFO FileScanRDD: Reading File path: file:///Users/zigzeug/Documents/GitHub/projet_big_data_cytech_25/data/raw/yellow_tripdata_2025-01.parquet, range: 23757201-31676268, partition values: [empty row]
[error] 25/11/30 15:42:53 INFO FileScanRDD: Reading File path: file:///Users/zigzeug/Documents/GitHub/projet_big_data_cytech_25/data/raw/yellow_tripdata_2025-01.parquet, range: 55433469-59158238, partition values: [empty row]
[error] 25/11/30 15:42:53 INFO FileScanRDD: Reading File path: file:///Users/zigzeug/Documents/GitHub/projet_big_data_cytech_25/data/raw/yellow_tripdata_2025-01.parquet, range: 7919067-15838134, partition values: [empty row]
[error] 25/11/30 15:42:53 INFO FileScanRDD: Reading File path: file:///Users/zigzeug/Documents/GitHub/projet_big_data_cytech_25/data/raw/yellow_tripdata_2025-01.parquet, range: 47514402-55433469, partition values: [empty row]
[error] 25/11/30 15:42:53 INFO FileScanRDD: Reading File path: file:///Users/zigzeug/Documents/GitHub/projet_big_data_cytech_25/data/raw/yellow_tripdata_2025-01.parquet, range: 31676268-39595335, partition values: [empty row]
[error] 25/11/30 15:42:53 INFO FileScanRDD: Reading File path: file:///Users/zigzeug/Documents/GitHub/projet_big_data_cytech_25/data/raw/yellow_tripdata_2025-01.parquet, range: 0-7919067, partition values: [empty row]
[error] 25/11/30 15:42:53 INFO FileScanRDD: Reading File path: file:///Users/zigzeug/Documents/GitHub/projet_big_data_cytech_25/data/raw/yellow_tripdata_2025-01.parquet, range: 39595335-47514402, partition values: [empty row]
[error] 25/11/30 15:42:53 INFO Executor: Finished task 6.0 in stage 7.0 (TID 25). 2184 bytes result sent to driver
[error] 25/11/30 15:42:53 INFO Executor: Finished task 7.0 in stage 7.0 (TID 26). 2270 bytes result sent to driver
[error] 25/11/30 15:42:53 INFO TaskSetManager: Finished task 6.0 in stage 7.0 (TID 25) in 36 ms on 10.188.186.236 (executor driver) (1/8)
[error] 25/11/30 15:42:53 INFO TaskSetManager: Finished task 7.0 in stage 7.0 (TID 26) in 36 ms on 10.188.186.236 (executor driver) (2/8)
[error] 25/11/30 15:42:53 INFO Executor: Finished task 3.0 in stage 7.0 (TID 22). 2227 bytes result sent to driver
[error] 25/11/30 15:42:53 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 22) in 40 ms on 10.188.186.236 (executor driver) (3/8)
[error] 25/11/30 15:42:53 INFO Executor: Finished task 4.0 in stage 7.0 (TID 23). 2184 bytes result sent to driver
[error] 25/11/30 15:42:53 INFO TaskSetManager: Finished task 4.0 in stage 7.0 (TID 23) in 43 ms on 10.188.186.236 (executor driver) (4/8)
[error] 25/11/30 15:42:53 INFO Executor: Finished task 0.0 in stage 7.0 (TID 19). 2184 bytes result sent to driver
[error] 25/11/30 15:42:53 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 19) in 47 ms on 10.188.186.236 (executor driver) (5/8)
[error] 25/11/30 15:42:53 INFO Executor: Finished task 2.0 in stage 7.0 (TID 21). 2184 bytes result sent to driver
[error] 25/11/30 15:42:53 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 21) in 48 ms on 10.188.186.236 (executor driver) (6/8)
[error] 25/11/30 15:42:53 INFO Executor: Finished task 1.0 in stage 7.0 (TID 20). 2227 bytes result sent to driver
[error] 25/11/30 15:42:53 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 20) in 50 ms on 10.188.186.236 (executor driver) (7/8)
[error] 25/11/30 15:42:53 INFO Executor: Finished task 5.0 in stage 7.0 (TID 24). 2227 bytes result sent to driver
[error] 25/11/30 15:42:53 INFO TaskSetManager: Finished task 5.0 in stage 7.0 (TID 24) in 50 ms on 10.188.186.236 (executor driver) (8/8)
[error] 25/11/30 15:42:53 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
[error] 25/11/30 15:42:53 INFO DAGScheduler: ShuffleMapStage 7 (count at DataValidation.scala:44) finished in 0.064 s
[error] 25/11/30 15:42:53 INFO DAGScheduler: looking for newly runnable stages
[error] 25/11/30 15:42:53 INFO DAGScheduler: running: HashSet()
[error] 25/11/30 15:42:53 INFO DAGScheduler: waiting: HashSet()
[error] 25/11/30 15:42:53 INFO DAGScheduler: failed: HashSet()
[error] 25/11/30 15:42:53 INFO SparkContext: Starting job: count at DataValidation.scala:44
[error] 25/11/30 15:42:53 INFO DAGScheduler: Got job 6 (count at DataValidation.scala:44) with 1 output partitions
[error] 25/11/30 15:42:53 INFO DAGScheduler: Final stage: ResultStage 9 (count at DataValidation.scala:44)
[error] 25/11/30 15:42:53 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)
[error] 25/11/30 15:42:53 INFO DAGScheduler: Missing parents: List()
[error] 25/11/30 15:42:53 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[22] at count at DataValidation.scala:44), which has no missing parents
[error] 25/11/30 15:42:53 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 13.1 KiB, free 2.2 GiB)
[error] 25/11/30 15:42:53 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 6.1 KiB, free 2.2 GiB)
[error] 25/11/30 15:42:53 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.188.186.236:51742 (size: 6.1 KiB, free: 2.2 GiB)
[error] 25/11/30 15:42:53 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1580
[error] 25/11/30 15:42:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[22] at count at DataValidation.scala:44) (first 15 tasks are for partitions Vector(0))
[error] 25/11/30 15:42:53 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
[error] 25/11/30 15:42:53 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 27) (10.188.186.236, executor driver, partition 0, NODE_LOCAL, 7695 bytes) 
[error] 25/11/30 15:42:53 INFO Executor: Running task 0.0 in stage 9.0 (TID 27)
[error] 25/11/30 15:42:53 INFO ShuffleBlockFetcherIterator: Getting 8 (480.0 B) non-empty blocks including 8 (480.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[error] 25/11/30 15:42:53 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[error] 25/11/30 15:42:53 INFO Executor: Finished task 0.0 in stage 9.0 (TID 27). 4084 bytes result sent to driver
[error] 25/11/30 15:42:53 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 27) in 12 ms on 10.188.186.236 (executor driver) (1/1)
[error] 25/11/30 15:42:53 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
[error] 25/11/30 15:42:53 INFO DAGScheduler: ResultStage 9 (count at DataValidation.scala:44) finished in 0.020 s
[error] 25/11/30 15:42:53 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[error] 25/11/30 15:42:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
[error] 25/11/30 15:42:53 INFO DAGScheduler: Job 6 finished: count at DataValidation.scala:44, took 0.022313 s
[info] Invalid records: 626055
[info] --- Starting Branch 1: Minio Storage ---
[info] Writing validated data to local path: temp_validated_data
[error] 25/11/30 15:42:53 INFO FileSourceStrategy: Pushed Filters: IsNotNull(passenger_count),IsNotNull(trip_distance),IsNotNull(total_amount),IsNotNull(tpep_pickup_datetime),IsNotNull(tpep_dropoff_datetime),GreaterThan(passenger_count,0),GreaterThanOrEqual(trip_distance,0.0),GreaterThanOrEqual(total_amount,0.0)
[error] 25/11/30 15:42:53 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(passenger_count#3L),isnotnull(trip_distance#4),isnotnull(total_amount#16),isnotnull(tpep_pickup_datetime#1),isnotnull(tpep_dropoff_datetime#2),(passenger_count#3L > 0),(trip_distance#4 >= 0.0),(total_amount#16 >= 0.0),(tpep_pickup_datetime#1 < tpep_dropoff_datetime#2)
[error] 25/11/30 15:42:53 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[error] 25/11/30 15:42:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[error] 25/11/30 15:42:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[error] 25/11/30 15:42:53 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[error] 25/11/30 15:42:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[error] 25/11/30 15:42:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[error] 25/11/30 15:42:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[error] 25/11/30 15:42:53 INFO CodeGenerator: Code generated in 66.684458 ms
[error] 25/11/30 15:42:53 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 202.8 KiB, free 2.2 GiB)
[error] 25/11/30 15:42:53 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 2.2 GiB)
[error] 25/11/30 15:42:53 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.188.186.236:51742 (size: 35.6 KiB, free: 2.2 GiB)
[error] 25/11/30 15:42:53 INFO SparkContext: Created broadcast 10 from parquet at DataValidation.scala:59
[error] 25/11/30 15:42:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 7919067 bytes, open cost is considered as scanning 4194304 bytes.
[error] 25/11/30 15:42:53 INFO SparkContext: Starting job: parquet at DataValidation.scala:59
[error] 25/11/30 15:42:53 INFO DAGScheduler: Got job 7 (parquet at DataValidation.scala:59) with 8 output partitions
[error] 25/11/30 15:42:53 INFO DAGScheduler: Final stage: ResultStage 10 (parquet at DataValidation.scala:59)
[error] 25/11/30 15:42:53 INFO DAGScheduler: Parents of final stage: List()
[error] 25/11/30 15:42:53 INFO DAGScheduler: Missing parents: List()
[error] 25/11/30 15:42:53 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[26] at parquet at DataValidation.scala:59), which has no missing parents
[error] 25/11/30 15:42:53 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 229.9 KiB, free 2.2 GiB)
[error] 25/11/30 15:42:53 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 80.2 KiB, free 2.2 GiB)
[error] 25/11/30 15:42:53 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.188.186.236:51742 (size: 80.2 KiB, free: 2.2 GiB)
[error] 25/11/30 15:42:53 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1580
[error] 25/11/30 15:42:53 INFO DAGScheduler: Submitting 8 missing tasks from ResultStage 10 (MapPartitionsRDD[26] at parquet at DataValidation.scala:59) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7))
[error] 25/11/30 15:42:53 INFO TaskSchedulerImpl: Adding task set 10.0 with 8 tasks resource profile 0
[error] 25/11/30 15:42:53 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 28) (10.188.186.236, executor driver, partition 0, PROCESS_LOCAL, 8363 bytes) 
[error] 25/11/30 15:42:53 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 29) (10.188.186.236, executor driver, partition 1, PROCESS_LOCAL, 8363 bytes) 
[error] 25/11/30 15:42:53 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 30) (10.188.186.236, executor driver, partition 2, PROCESS_LOCAL, 8363 bytes) 
[error] 25/11/30 15:42:53 INFO TaskSetManager: Starting task 3.0 in stage 10.0 (TID 31) (10.188.186.236, executor driver, partition 3, PROCESS_LOCAL, 8363 bytes) 
[error] 25/11/30 15:42:53 INFO TaskSetManager: Starting task 4.0 in stage 10.0 (TID 32) (10.188.186.236, executor driver, partition 4, PROCESS_LOCAL, 8363 bytes) 
[error] 25/11/30 15:42:53 INFO TaskSetManager: Starting task 5.0 in stage 10.0 (TID 33) (10.188.186.236, executor driver, partition 5, PROCESS_LOCAL, 8363 bytes) 
[error] 25/11/30 15:42:53 INFO TaskSetManager: Starting task 6.0 in stage 10.0 (TID 34) (10.188.186.236, executor driver, partition 6, PROCESS_LOCAL, 8363 bytes) 
[error] 25/11/30 15:42:53 INFO TaskSetManager: Starting task 7.0 in stage 10.0 (TID 35) (10.188.186.236, executor driver, partition 7, PROCESS_LOCAL, 8363 bytes) 
[error] 25/11/30 15:42:53 INFO Executor: Running task 1.0 in stage 10.0 (TID 29)
[error] 25/11/30 15:42:53 INFO Executor: Running task 2.0 in stage 10.0 (TID 30)
[error] 25/11/30 15:42:53 INFO Executor: Running task 0.0 in stage 10.0 (TID 28)
[error] 25/11/30 15:42:53 INFO Executor: Running task 3.0 in stage 10.0 (TID 31)
[error] 25/11/30 15:42:53 INFO Executor: Running task 4.0 in stage 10.0 (TID 32)
[error] 25/11/30 15:42:53 INFO Executor: Running task 5.0 in stage 10.0 (TID 33)
[error] 25/11/30 15:42:53 INFO Executor: Running task 6.0 in stage 10.0 (TID 34)
[error] 25/11/30 15:42:53 INFO Executor: Running task 7.0 in stage 10.0 (TID 35)
[error] 25/11/30 15:42:53 INFO CodeGenerator: Code generated in 34.876361 ms
[error] 25/11/30 15:42:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[error] 25/11/30 15:42:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[error] 25/11/30 15:42:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[error] 25/11/30 15:42:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[error] 25/11/30 15:42:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[error] 25/11/30 15:42:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[error] 25/11/30 15:42:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[error] 25/11/30 15:42:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[error] 25/11/30 15:42:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[error] 25/11/30 15:42:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[error] 25/11/30 15:42:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[error] 25/11/30 15:42:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[error] 25/11/30 15:42:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[error] 25/11/30 15:42:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[error] 25/11/30 15:42:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[error] 25/11/30 15:42:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[error] 25/11/30 15:42:53 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[error] 25/11/30 15:42:53 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[error] 25/11/30 15:42:53 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[error] 25/11/30 15:42:53 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[error] 25/11/30 15:42:53 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[error] 25/11/30 15:42:53 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[error] 25/11/30 15:42:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[error] 25/11/30 15:42:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[error] 25/11/30 15:42:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[error] 25/11/30 15:42:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[error] 25/11/30 15:42:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[error] 25/11/30 15:42:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[error] 25/11/30 15:42:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[error] 25/11/30 15:42:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[error] 25/11/30 15:42:53 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[error] 25/11/30 15:42:53 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[error] 25/11/30 15:42:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[error] 25/11/30 15:42:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[error] 25/11/30 15:42:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[error] 25/11/30 15:42:53 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[error] 25/11/30 15:42:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[error] 25/11/30 15:42:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[error] 25/11/30 15:42:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[error] 25/11/30 15:42:53 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[error] 25/11/30 15:42:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[error] 25/11/30 15:42:53 INFO FileScanRDD: Reading File path: file:///Users/zigzeug/Documents/GitHub/projet_big_data_cytech_25/data/raw/yellow_tripdata_2025-01.parquet, range: 47514402-55433469, partition values: [empty row]
[error] 25/11/30 15:42:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[error] 25/11/30 15:42:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[error] 25/11/30 15:42:53 INFO FileScanRDD: Reading File path: file:///Users/zigzeug/Documents/GitHub/projet_big_data_cytech_25/data/raw/yellow_tripdata_2025-01.parquet, range: 7919067-15838134, partition values: [empty row]
[error] 25/11/30 15:42:53 INFO FileScanRDD: Reading File path: file:///Users/zigzeug/Documents/GitHub/projet_big_data_cytech_25/data/raw/yellow_tripdata_2025-01.parquet, range: 31676268-39595335, partition values: [empty row]
[error] 25/11/30 15:42:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[error] 25/11/30 15:42:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[error] 25/11/30 15:42:53 INFO FileScanRDD: Reading File path: file:///Users/zigzeug/Documents/GitHub/projet_big_data_cytech_25/data/raw/yellow_tripdata_2025-01.parquet, range: 39595335-47514402, partition values: [empty row]
[error] 25/11/30 15:42:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[error] 25/11/30 15:42:53 INFO FileScanRDD: Reading File path: file:///Users/zigzeug/Documents/GitHub/projet_big_data_cytech_25/data/raw/yellow_tripdata_2025-01.parquet, range: 23757201-31676268, partition values: [empty row]
[error] 25/11/30 15:42:53 INFO FileScanRDD: Reading File path: file:///Users/zigzeug/Documents/GitHub/projet_big_data_cytech_25/data/raw/yellow_tripdata_2025-01.parquet, range: 15838134-23757201, partition values: [empty row]
[error] 25/11/30 15:42:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[error] 25/11/30 15:42:53 INFO FileScanRDD: Reading File path: file:///Users/zigzeug/Documents/GitHub/projet_big_data_cytech_25/data/raw/yellow_tripdata_2025-01.parquet, range: 55433469-59158238, partition values: [empty row]
[error] 25/11/30 15:42:53 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[error] 25/11/30 15:42:53 INFO CodecConfig: Compression: SNAPPY
[error] 25/11/30 15:42:53 INFO CodecConfig: Compression: SNAPPY
[error] 25/11/30 15:42:53 INFO FilterCompat: Filtering using predicate: and(and(and(and(and(noteq(passenger_count, null), noteq(trip_distance, null)), noteq(total_amount, null)), gt(passenger_count, 0)), gteq(trip_distance, 0.0)), gteq(total_amount, 0.0))
[error] 25/11/30 15:42:53 INFO FilterCompat: Filtering using predicate: and(and(and(and(and(noteq(passenger_count, null), noteq(trip_distance, null)), noteq(total_amount, null)), gt(passenger_count, 0)), gteq(trip_distance, 0.0)), gteq(total_amount, 0.0))
[error] 25/11/30 15:42:53 INFO FilterCompat: Filtering using predicate: and(and(and(and(and(noteq(passenger_count, null), noteq(trip_distance, null)), noteq(total_amount, null)), gt(passenger_count, 0)), gteq(trip_distance, 0.0)), gteq(total_amount, 0.0))
[error] 25/11/30 15:42:53 INFO FilterCompat: Filtering using predicate: and(and(and(and(and(noteq(passenger_count, null), noteq(trip_distance, null)), noteq(total_amount, null)), gt(passenger_count, 0)), gteq(trip_distance, 0.0)), gteq(total_amount, 0.0))
[error] 25/11/30 15:42:53 INFO CodecPool: Got brand-new decompressor [.zstd]
[error] 25/11/30 15:42:53 INFO FilterCompat: Filtering using predicate: and(and(and(and(and(noteq(passenger_count, null), noteq(trip_distance, null)), noteq(total_amount, null)), gt(passenger_count, 0)), gteq(trip_distance, 0.0)), gteq(total_amount, 0.0))
[error] 25/11/30 15:42:53 INFO FilterCompat: Filtering using predicate: and(and(and(and(and(noteq(passenger_count, null), noteq(trip_distance, null)), noteq(total_amount, null)), gt(passenger_count, 0)), gteq(trip_distance, 0.0)), gteq(total_amount, 0.0))
[error] 25/11/30 15:42:53 INFO FilterCompat: Filtering using predicate: and(and(and(and(and(noteq(passenger_count, null), noteq(trip_distance, null)), noteq(total_amount, null)), gt(passenger_count, 0)), gteq(trip_distance, 0.0)), gteq(total_amount, 0.0))
[error] 25/11/30 15:42:53 INFO CodecPool: Got brand-new decompressor [.zstd]
[error] 25/11/30 15:42:53 INFO CodecPool: Got brand-new decompressor [.zstd]
[error] 25/11/30 15:42:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202511301542533566165407053269679_0010_m_000007_35
[error] 25/11/30 15:42:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202511301542533566165407053269679_0010_m_000006_34
[error] 25/11/30 15:42:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202511301542533566165407053269679_0010_m_000002_30
[error] 25/11/30 15:42:53 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202511301542533566165407053269679_0010_m_000004_32
[error] 25/11/30 15:42:53 INFO ColumnIndexFilter: No offset index for column passenger_count is available; Unable to do filtering
[error] 25/11/30 15:42:53 INFO ColumnIndexFilter: No offset index for column passenger_count is available; Unable to do filtering
[error] 25/11/30 15:42:53 INFO ColumnIndexFilter: No offset index for column passenger_count is available; Unable to do filtering
[error] 25/11/30 15:42:53 INFO Executor: Finished task 2.0 in stage 10.0 (TID 30). 2963 bytes result sent to driver
[error] 25/11/30 15:42:53 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 30) in 165 ms on 10.188.186.236 (executor driver) (1/8)
[error] 25/11/30 15:42:53 INFO Executor: Finished task 7.0 in stage 10.0 (TID 35). 2963 bytes result sent to driver
[error] 25/11/30 15:42:53 INFO Executor: Finished task 4.0 in stage 10.0 (TID 32). 2963 bytes result sent to driver
[error] 25/11/30 15:42:53 INFO TaskSetManager: Finished task 7.0 in stage 10.0 (TID 35) in 170 ms on 10.188.186.236 (executor driver) (2/8)
[error] 25/11/30 15:42:53 INFO TaskSetManager: Finished task 4.0 in stage 10.0 (TID 32) in 172 ms on 10.188.186.236 (executor driver) (3/8)
[error] 25/11/30 15:42:53 INFO Executor: Finished task 6.0 in stage 10.0 (TID 34). 2963 bytes result sent to driver
[error] 25/11/30 15:42:53 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[error] 25/11/30 15:42:53 INFO TaskSetManager: Finished task 6.0 in stage 10.0 (TID 34) in 172 ms on 10.188.186.236 (executor driver) (4/8)
[error] 25/11/30 15:42:53 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 10.188.186.236:51742 in memory (size: 34.6 KiB, free: 2.2 GiB)
[error] 25/11/30 15:42:53 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 10.188.186.236:51742 in memory (size: 6.1 KiB, free: 2.2 GiB)
[error] 25/11/30 15:42:53 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 10.188.186.236:51742 in memory (size: 7.9 KiB, free: 2.2 GiB)
[error] 25/11/30 15:42:53 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[error] {
[error]   "type" : "struct",
[error]   "fields" : [ {
[error]     "name" : "VendorID",
[error]     "type" : "integer",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "tpep_pickup_datetime",
[error]     "type" : "timestamp_ntz",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "tpep_dropoff_datetime",
[error]     "type" : "timestamp_ntz",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "passenger_count",
[error]     "type" : "long",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "trip_distance",
[error]     "type" : "double",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "RatecodeID",
[error]     "type" : "long",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "store_and_fwd_flag",
[error]     "type" : "string",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "PULocationID",
[error]     "type" : "integer",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "DOLocationID",
[error]     "type" : "integer",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "payment_type",
[error]     "type" : "long",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "fare_amount",
[error]     "type" : "double",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "extra",
[error]     "type" : "double",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "mta_tax",
[error]     "type" : "double",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "tip_amount",
[error]     "type" : "double",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "tolls_amount",
[error]     "type" : "double",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "improvement_surcharge",
[error]     "type" : "double",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "total_amount",
[error]     "type" : "double",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "congestion_surcharge",
[error]     "type" : "double",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "Airport_fee",
[error]     "type" : "double",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "cbd_congestion_fee",
[error]     "type" : "double",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   } ]
[error] }
[error] and corresponding Parquet message type:
[error] message spark_schema {
[error]   optional int32 VendorID;
[error]   optional int64 tpep_pickup_datetime (TIMESTAMP(MICROS,false));
[error]   optional int64 tpep_dropoff_datetime (TIMESTAMP(MICROS,false));
[error]   optional int64 passenger_count;
[error]   optional double trip_distance;
[error]   optional int64 RatecodeID;
[error]   optional binary store_and_fwd_flag (STRING);
[error]   optional int32 PULocationID;
[error]   optional int32 DOLocationID;
[error]   optional int64 payment_type;
[error]   optional double fare_amount;
[error]   optional double extra;
[error]   optional double mta_tax;
[error]   optional double tip_amount;
[error]   optional double tolls_amount;
[error]   optional double improvement_surcharge;
[error]   optional double total_amount;
[error]   optional double congestion_surcharge;
[error]   optional double Airport_fee;
[error]   optional double cbd_congestion_fee;
[error] }
[error]        
[error] 25/11/30 15:42:53 INFO CodecConfig: Compression: SNAPPY
[error] 25/11/30 15:42:53 INFO CodecConfig: Compression: SNAPPY
[error] 25/11/30 15:42:53 INFO CodecConfig: Compression: SNAPPY
[error] 25/11/30 15:42:53 INFO CodecConfig: Compression: SNAPPY
[error] 25/11/30 15:42:53 INFO CodecConfig: Compression: SNAPPY
[error] 25/11/30 15:42:53 INFO CodecConfig: Compression: SNAPPY
[error] 25/11/30 15:42:53 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[error] 25/11/30 15:42:53 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[error] 25/11/30 15:42:53 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[error] 25/11/30 15:42:53 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[error] {
[error]   "type" : "struct",
[error]   "fields" : [ {
[error]     "name" : "VendorID",
[error]     "type" : "integer",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "tpep_pickup_datetime",
[error]     "type" : "timestamp_ntz",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "tpep_dropoff_datetime",
[error]     "type" : "timestamp_ntz",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "passenger_count",
[error]     "type" : "long",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "trip_distance",
[error]     "type" : "double",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "RatecodeID",
[error]     "type" : "long",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "store_and_fwd_flag",
[error]     "type" : "string",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "PULocationID",
[error]     "type" : "integer",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "DOLocationID",
[error]     "type" : "integer",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "payment_type",
[error]     "type" : "long",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "fare_amount",
[error]     "type" : "double",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "extra",
[error]     "type" : "double",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "mta_tax",
[error]     "type" : "double",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "tip_amount",
[error]     "type" : "double",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "tolls_amount",
[error]     "type" : "double",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "improvement_surcharge",
[error]     "type" : "double",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "total_amount",
[error]     "type" : "double",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "congestion_surcharge",
[error]     "type" : "double",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "Airport_fee",
[error]     "type" : "double",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "cbd_congestion_fee",
[error]     "type" : "double",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   } ]
[error] }
[error] and corresponding Parquet message type:
[error] message spark_schema {
[error]   optional int32 VendorID;
[error]   optional int64 tpep_pickup_datetime (TIMESTAMP(MICROS,false));
[error]   optional int64 tpep_dropoff_datetime (TIMESTAMP(MICROS,false));
[error]   optional int64 passenger_count;
[error]   optional double trip_distance;
[error]   optional int64 RatecodeID;
[error]   optional binary store_and_fwd_flag (STRING);
[error]   optional int32 PULocationID;
[error]   optional int32 DOLocationID;
[error]   optional int64 payment_type;
[error]   optional double fare_amount;
[error]   optional double extra;
[error]   optional double mta_tax;
[error]   optional double tip_amount;
[error]   optional double tolls_amount;
[error]   optional double improvement_surcharge;
[error]   optional double total_amount;
[error]   optional double congestion_surcharge;
[error]   optional double Airport_fee;
[error]   optional double cbd_congestion_fee;
[error] }
[error]        
[error] 25/11/30 15:42:53 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[error] {
[error]   "type" : "struct",
[error]   "fields" : [ {
[error]     "name" : "VendorID",
[error]     "type" : "integer",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "tpep_pickup_datetime",
[error]     "type" : "timestamp_ntz",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "tpep_dropoff_datetime",
[error]     "type" : "timestamp_ntz",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "passenger_count",
[error]     "type" : "long",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "trip_distance",
[error]     "type" : "double",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "RatecodeID",
[error]     "type" : "long",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "store_and_fwd_flag",
[error]     "type" : "string",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "PULocationID",
[error]     "type" : "integer",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "DOLocationID",
[error]     "type" : "integer",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "payment_type",
[error]     "type" : "long",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "fare_amount",
[error]     "type" : "double",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "extra",
[error]     "type" : "double",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "mta_tax",
[error]     "type" : "double",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "tip_amount",
[error]     "type" : "double",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "tolls_amount",
[error]     "type" : "double",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "improvement_surcharge",
[error]     "type" : "double",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "total_amount",
[error]     "type" : "double",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "congestion_surcharge",
[error]     "type" : "double",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "Airport_fee",
[error]     "type" : "double",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "cbd_congestion_fee",
[error]     "type" : "double",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   } ]
[error] }
[error] and corresponding Parquet message type:
[error] message spark_schema {
[error]   optional int32 VendorID;
[error]   optional int64 tpep_pickup_datetime (TIMESTAMP(MICROS,false));
[error]   optional int64 tpep_dropoff_datetime (TIMESTAMP(MICROS,false));
[error]   optional int64 passenger_count;
[error]   optional double trip_distance;
[error]   optional int64 RatecodeID;
[error]   optional binary store_and_fwd_flag (STRING);
[error]   optional int32 PULocationID;
[error]   optional int32 DOLocationID;
[error]   optional int64 payment_type;
[error]   optional double fare_amount;
[error]   optional double extra;
[error]   optional double mta_tax;
[error]   optional double tip_amount;
[error]   optional double tolls_amount;
[error]   optional double improvement_surcharge;
[error]   optional double total_amount;
[error]   optional double congestion_surcharge;
[error]   optional double Airport_fee;
[error]   optional double cbd_congestion_fee;
[error] }
[error]        
[error] 25/11/30 15:42:53 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[error] {
[error]   "type" : "struct",
[error]   "fields" : [ {
[error]     "name" : "VendorID",
[error]     "type" : "integer",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "tpep_pickup_datetime",
[error]     "type" : "timestamp_ntz",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "tpep_dropoff_datetime",
[error]     "type" : "timestamp_ntz",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "passenger_count",
[error]     "type" : "long",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "trip_distance",
[error]     "type" : "double",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "RatecodeID",
[error]     "type" : "long",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "store_and_fwd_flag",
[error]     "type" : "string",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "PULocationID",
[error]     "type" : "integer",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "DOLocationID",
[error]     "type" : "integer",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "payment_type",
[error]     "type" : "long",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "fare_amount",
[error]     "type" : "double",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "extra",
[error]     "type" : "double",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "mta_tax",
[error]     "type" : "double",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "tip_amount",
[error]     "type" : "double",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "tolls_amount",
[error]     "type" : "double",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "improvement_surcharge",
[error]     "type" : "double",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "total_amount",
[error]     "type" : "double",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "congestion_surcharge",
[error]     "type" : "double",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "Airport_fee",
[error]     "type" : "double",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   }, {
[error]     "name" : "cbd_congestion_fee",
[error]     "type" : "double",
[error]     "nullable" : true,
[error]     "metadata" : { }
[error]   } ]
[error] }
[error] and corresponding Parquet message type:
[error] message spark_schema {
[error]   optional int32 VendorID;
[error]   optional int64 tpep_pickup_datetime (TIMESTAMP(MICROS,false));
[error]   optional int64 tpep_dropoff_datetime (TIMESTAMP(MICROS,false));
[error]   optional int64 passenger_count;
[error]   optional double trip_distance;
[error]   optional int64 RatecodeID;
[error]   optional binary store_and_fwd_flag (STRING);
[error]   optional int32 PULocationID;
[error]   optional int32 DOLocationID;
[error]   optional int64 payment_type;
[error]   optional double fare_amount;
[error]   optional double extra;
[error]   optional double mta_tax;
[error]   optional double tip_amount;
[error]   optional double tolls_amount;
[error]   optional double improvement_surcharge;
[error]   optional double total_amount;
[error]   optional double congestion_surcharge;
[error]   optional double Airport_fee;
[error]   optional double cbd_congestion_fee;
[error] }
[error]        
[error] 25/11/30 15:42:53 INFO CodecPool: Got brand-new compressor [.snappy]
[error] 25/11/30 15:42:54 INFO CodecPool: Got brand-new compressor [.snappy]
[error] 25/11/30 15:42:54 INFO CodecPool: Got brand-new compressor [.snappy]
[error] 25/11/30 15:42:54 INFO CodecPool: Got brand-new compressor [.snappy]
[error] 25/11/30 15:42:54 INFO FileScanRDD: Reading File path: file:///Users/zigzeug/Documents/GitHub/projet_big_data_cytech_25/data/raw/yellow_tripdata_2025-01.parquet, range: 0-7919067, partition values: [empty row]
[error] 25/11/30 15:42:54 INFO FilterCompat: Filtering using predicate: and(and(and(and(and(noteq(passenger_count, null), noteq(trip_distance, null)), noteq(total_amount, null)), gt(passenger_count, 0)), gteq(trip_distance, 0.0)), gteq(total_amount, 0.0))
[error] 25/11/30 15:42:54 INFO FileOutputCommitter: Saved output of task 'attempt_202511301542533566165407053269679_0010_m_000000_28' to file:/Users/zigzeug/Documents/GitHub/projet_big_data_cytech_25/ex02_data_ingestion/temp_validated_data/_temporary/0/task_202511301542533566165407053269679_0010_m_000000
[error] 25/11/30 15:42:54 INFO SparkHadoopMapRedUtil: attempt_202511301542533566165407053269679_0010_m_000000_28: Committed. Elapsed time: 2 ms.
[error] 25/11/30 15:42:54 INFO Executor: Finished task 0.0 in stage 10.0 (TID 28). 3006 bytes result sent to driver
[error] 25/11/30 15:42:54 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 28) in 557 ms on 10.188.186.236 (executor driver) (5/8)
[error] 25/11/30 15:43:01 INFO FileOutputCommitter: Saved output of task 'attempt_202511301542533566165407053269679_0010_m_000005_33' to file:/Users/zigzeug/Documents/GitHub/projet_big_data_cytech_25/ex02_data_ingestion/temp_validated_data/_temporary/0/task_202511301542533566165407053269679_0010_m_000005
[error] 25/11/30 15:43:01 INFO SparkHadoopMapRedUtil: attempt_202511301542533566165407053269679_0010_m_000005_33: Committed. Elapsed time: 2 ms.
[error] 25/11/30 15:43:01 INFO Executor: Finished task 5.0 in stage 10.0 (TID 33). 3092 bytes result sent to driver
[error] 25/11/30 15:43:01 INFO TaskSetManager: Finished task 5.0 in stage 10.0 (TID 33) in 7697 ms on 10.188.186.236 (executor driver) (6/8)
[error] 25/11/30 15:43:02 INFO FileOutputCommitter: Saved output of task 'attempt_202511301542533566165407053269679_0010_m_000003_31' to file:/Users/zigzeug/Documents/GitHub/projet_big_data_cytech_25/ex02_data_ingestion/temp_validated_data/_temporary/0/task_202511301542533566165407053269679_0010_m_000003
[error] 25/11/30 15:43:02 INFO SparkHadoopMapRedUtil: attempt_202511301542533566165407053269679_0010_m_000003_31: Committed. Elapsed time: 2 ms.
[error] 25/11/30 15:43:02 INFO Executor: Finished task 3.0 in stage 10.0 (TID 31). 3092 bytes result sent to driver
[error] 25/11/30 15:43:02 INFO TaskSetManager: Finished task 3.0 in stage 10.0 (TID 31) in 8810 ms on 10.188.186.236 (executor driver) (7/8)
[error] 25/11/30 15:43:02 INFO FileOutputCommitter: Saved output of task 'attempt_202511301542533566165407053269679_0010_m_000001_29' to file:/Users/zigzeug/Documents/GitHub/projet_big_data_cytech_25/ex02_data_ingestion/temp_validated_data/_temporary/0/task_202511301542533566165407053269679_0010_m_000001
[error] 25/11/30 15:43:02 INFO SparkHadoopMapRedUtil: attempt_202511301542533566165407053269679_0010_m_000001_29: Committed. Elapsed time: 2 ms.
[error] 25/11/30 15:43:02 INFO Executor: Finished task 1.0 in stage 10.0 (TID 29). 3092 bytes result sent to driver
[error] 25/11/30 15:43:02 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 29) in 9029 ms on 10.188.186.236 (executor driver) (8/8)
[error] 25/11/30 15:43:02 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
[error] 25/11/30 15:43:02 INFO DAGScheduler: ResultStage 10 (parquet at DataValidation.scala:59) finished in 9.094 s
[error] 25/11/30 15:43:02 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[error] 25/11/30 15:43:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
[error] 25/11/30 15:43:02 INFO DAGScheduler: Job 7 finished: parquet at DataValidation.scala:59, took 9.099475 s
[error] 25/11/30 15:43:02 INFO FileFormatWriter: Start to commit write Job 4b0b0950-73fe-469e-995f-6d03b9962c2b.
[error] 25/11/30 15:43:02 INFO FileFormatWriter: Write Job 4b0b0950-73fe-469e-995f-6d03b9962c2b committed. Elapsed time: 73 ms.
[error] 25/11/30 15:43:02 INFO FileFormatWriter: Finished processing stats for write job 4b0b0950-73fe-469e-995f-6d03b9962c2b.
[info] Uploading to Minio bucket 'warehouse' at 'validated/yellow_tripdata_2025-01'...
[info] Error: Failed to connect to localhost/0:0:0:0:0:0:0:1:9000
[error] java.net.ConnectException: Failed to connect to localhost/0:0:0:0:0:0:0:1:9000
[error] 	at okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:297)
[error] 	at okhttp3.internal.connection.RealConnection.connect(RealConnection.kt:207)
[error] 	at okhttp3.internal.connection.ExchangeFinder.findConnection(ExchangeFinder.kt:226)
[error] 	at okhttp3.internal.connection.ExchangeFinder.findHealthyConnection(ExchangeFinder.kt:106)
[error] 	at okhttp3.internal.connection.ExchangeFinder.find(ExchangeFinder.kt:74)
[error] 	at okhttp3.internal.connection.RealCall.initExchange$okhttp(RealCall.kt:255)
[error] 	at okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:32)
[error] 	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)
[error] 	at okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)
[error] 	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)
[error] 	at okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)
[error] 	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)
[error] 	at okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)
[error] 	at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)
[error] 	at okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)
[error] 	at okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)
[error] 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[error] 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[error] 	at java.base/java.lang.Thread.run(Thread.java:829)
[error] 	Suppressed: java.net.ConnectException: Failed to connect to localhost/127.0.0.1:9000
[error] 		... 19 more
[error] 	Caused by: java.net.ConnectException: Connection refused (Connection refused)
[error] 		at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
[error] 		at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
[error] 		at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
[error] 		at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
[error] 		at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
[error] 		at java.base/java.net.Socket.connect(Socket.java:609)
[error] 		at okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)
[error] 		at okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)
[error] 		... 18 more
[error] Caused by: java.net.ConnectException: Connection refused (Connection refused)
[error] 	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
[error] 	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
[error] 	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
[error] 	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
[error] 	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
[error] 	at java.base/java.net.Socket.connect(Socket.java:609)
[error] 	at okhttp3.internal.platform.Platform.connectSocket(Platform.kt:128)
[error] 	at okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.kt:295)
[error] 	... 18 more
[error] 25/11/30 15:43:03 INFO SparkContext: SparkContext is stopping with exitCode 0.
[error] 25/11/30 15:43:03 INFO SparkUI: Stopped Spark web UI at http://10.188.186.236:4040
[error] 25/11/30 15:43:03 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[error] 25/11/30 15:43:03 INFO MemoryStore: MemoryStore cleared
[error] 25/11/30 15:43:03 INFO BlockManager: BlockManager stopped
[error] 25/11/30 15:43:03 INFO BlockManagerMaster: BlockManagerMaster stopped
[error] 25/11/30 15:43:03 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[error] 25/11/30 15:43:03 INFO SparkContext: Successfully stopped SparkContext
[error] 25/11/30 15:44:03 INFO ShutdownHookManager: Shutdown hook called
[error] 25/11/30 15:44:03 INFO ShutdownHookManager: Deleting directory /private/var/folders/h1/pkmwsfd12hz6_rnssw0zx33w0000gn/T/spark-51a5c755-4237-40bd-aaac-077942238fc6
[success] Total time: 88 s (0:01:28.0), completed Nov 30, 2025, 3:44:03 PM
